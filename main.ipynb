{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10776e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Popen.terminate of <Popen: returncode: None args: ['./llama/llama-server', '-m', './llama/model...>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import atexit\n",
    "import threading\n",
    "import requests\n",
    "import sseclient\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:5175/v1/chat/completions\"\n",
    "\n",
    "process = subprocess.Popen([\n",
    "    \"./llama/llama-server\",\n",
    "    \"-m\", \"./llama/models/Qwen3-14B-Q5_K_M.gguf\",\n",
    "    \"-b\", \"512\",\n",
    "    \"--n-gpu-layers\", \"41\",\n",
    "    \"-t\", \"6\",\n",
    "    \"--port\", \"5175\",\n",
    "    \"--verbose\"\n",
    "], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, text=True, encoding=\"utf-8\")\n",
    "\n",
    "atexit.register(process.terminate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c899c473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./llama/llama-server: /lib64/libcurl.so.4: no version information available (required by ./llama/llama-server)\n",
      "load_backend: loaded RPC backend from /home/qbsoon/repos/LMnGen/llama/libggml-rpc.so\n",
      "ggml_vulkan: Found 1 Vulkan devices:\n",
      "ggml_vulkan: 0 = NVIDIA GeForce RTX 4070 Ti SUPER (NVIDIA) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\n",
      "load_backend: loaded Vulkan backend from /home/qbsoon/repos/LMnGen/llama/libggml-vulkan.so\n",
      "load_backend: loaded CPU backend from /home/qbsoon/repos/LMnGen/llama/libggml-cpu-icelake.so\n",
      "build: 6432 (ae355f6f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
      "system info: n_threads = 6, n_threads_batch = 6, total_threads = 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "system_info: n_threads = 6 (n_threads_batch = 6) / 12 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "\n",
      "main: binding port with default address family\n",
      "main: HTTP server is listening, hostname: 127.0.0.1, port: 5175, http threads: 11\n",
      "main: loading model\n",
      "srv    load_model: loading model './llama/models/Qwen3-14B-Q5_K_M.gguf'\n",
      "llama_model_load_from_file_impl: using device Vulkan0 (NVIDIA GeForce RTX 4070 Ti SUPER) - 13782 MiB free\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 443 tensors from ./llama/models/Qwen3-14B-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen3\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 14B\n",
      "llama_model_loader: - kv   6:                          qwen3.block_count u32              = 40\n",
      "llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\n",
      "llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 17408\n",
      "llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  27:                          general.file_type u32              = 17\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q5_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 9.79 GiB (5.69 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 151643 ('<|endoftext|>')\n",
      "load:   - 151645 ('<|im_end|>')\n",
      "load:   - 151662 ('<|fim_pad|>')\n",
      "load:   - 151663 ('<|repo_name|>')\n",
      "load:   - 151664 ('<|file_sep|>')\n",
      "load: special tokens cache size = 26\n",
      "load: token to piece cache size = 0.9311 MB\n",
      "print_info: arch             = qwen3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 40960\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 40\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 5\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 17408\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 40960\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 14B\n",
      "print_info: model params     = 14.77 B\n",
      "print_info: general.name     = Qwen3 14B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  37 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  38 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  39 assigned to device Vulkan0, is_swa = 0\n",
      "load_tensors: layer  40 assigned to device Vulkan0, is_swa = 0\n",
      "create_tensor: loading tensor token_embd.weight\n",
      "create_tensor: loading tensor output_norm.weight\n",
      "create_tensor: loading tensor output.weight\n",
      "create_tensor: loading tensor blk.0.attn_norm.weight\n",
      "create_tensor: loading tensor blk.0.attn_q.weight\n",
      "create_tensor: loading tensor blk.0.attn_k.weight\n",
      "create_tensor: loading tensor blk.0.attn_v.weight\n",
      "create_tensor: loading tensor blk.0.attn_output.weight\n",
      "create_tensor: loading tensor blk.0.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.0.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.0.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.0.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.0.ffn_down.weight\n",
      "create_tensor: loading tensor blk.0.ffn_up.weight\n",
      "create_tensor: loading tensor blk.1.attn_norm.weight\n",
      "create_tensor: loading tensor blk.1.attn_q.weight\n",
      "create_tensor: loading tensor blk.1.attn_k.weight\n",
      "create_tensor: loading tensor blk.1.attn_v.weight\n",
      "create_tensor: loading tensor blk.1.attn_output.weight\n",
      "create_tensor: loading tensor blk.1.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.1.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.1.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.1.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.1.ffn_down.weight\n",
      "create_tensor: loading tensor blk.1.ffn_up.weight\n",
      "create_tensor: loading tensor blk.2.attn_norm.weight\n",
      "create_tensor: loading tensor blk.2.attn_q.weight\n",
      "create_tensor: loading tensor blk.2.attn_k.weight\n",
      "create_tensor: loading tensor blk.2.attn_v.weight\n",
      "create_tensor: loading tensor blk.2.attn_output.weight\n",
      "create_tensor: loading tensor blk.2.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.2.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.2.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.2.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.2.ffn_down.weight\n",
      "create_tensor: loading tensor blk.2.ffn_up.weight\n",
      "create_tensor: loading tensor blk.3.attn_norm.weight\n",
      "create_tensor: loading tensor blk.3.attn_q.weight\n",
      "create_tensor: loading tensor blk.3.attn_k.weight\n",
      "create_tensor: loading tensor blk.3.attn_v.weight\n",
      "create_tensor: loading tensor blk.3.attn_output.weight\n",
      "create_tensor: loading tensor blk.3.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.3.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.3.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.3.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.3.ffn_down.weight\n",
      "create_tensor: loading tensor blk.3.ffn_up.weight\n",
      "create_tensor: loading tensor blk.4.attn_norm.weight\n",
      "create_tensor: loading tensor blk.4.attn_q.weight\n",
      "create_tensor: loading tensor blk.4.attn_k.weight\n",
      "create_tensor: loading tensor blk.4.attn_v.weight\n",
      "create_tensor: loading tensor blk.4.attn_output.weight\n",
      "create_tensor: loading tensor blk.4.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.4.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.4.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.4.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.4.ffn_down.weight\n",
      "create_tensor: loading tensor blk.4.ffn_up.weight\n",
      "create_tensor: loading tensor blk.5.attn_norm.weight\n",
      "create_tensor: loading tensor blk.5.attn_q.weight\n",
      "create_tensor: loading tensor blk.5.attn_k.weight\n",
      "create_tensor: loading tensor blk.5.attn_v.weight\n",
      "create_tensor: loading tensor blk.5.attn_output.weight\n",
      "create_tensor: loading tensor blk.5.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.5.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.5.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.5.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.5.ffn_down.weight\n",
      "create_tensor: loading tensor blk.5.ffn_up.weight\n",
      "create_tensor: loading tensor blk.6.attn_norm.weight\n",
      "create_tensor: loading tensor blk.6.attn_q.weight\n",
      "create_tensor: loading tensor blk.6.attn_k.weight\n",
      "create_tensor: loading tensor blk.6.attn_v.weight\n",
      "create_tensor: loading tensor blk.6.attn_output.weight\n",
      "create_tensor: loading tensor blk.6.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.6.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.6.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.6.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.6.ffn_down.weight\n",
      "create_tensor: loading tensor blk.6.ffn_up.weight\n",
      "create_tensor: loading tensor blk.7.attn_norm.weight\n",
      "create_tensor: loading tensor blk.7.attn_q.weight\n",
      "create_tensor: loading tensor blk.7.attn_k.weight\n",
      "create_tensor: loading tensor blk.7.attn_v.weight\n",
      "create_tensor: loading tensor blk.7.attn_output.weight\n",
      "create_tensor: loading tensor blk.7.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.7.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.7.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.7.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.7.ffn_down.weight\n",
      "create_tensor: loading tensor blk.7.ffn_up.weight\n",
      "create_tensor: loading tensor blk.8.attn_norm.weight\n",
      "create_tensor: loading tensor blk.8.attn_q.weight\n",
      "create_tensor: loading tensor blk.8.attn_k.weight\n",
      "create_tensor: loading tensor blk.8.attn_v.weight\n",
      "create_tensor: loading tensor blk.8.attn_output.weight\n",
      "create_tensor: loading tensor blk.8.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.8.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.8.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.8.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.8.ffn_down.weight\n",
      "create_tensor: loading tensor blk.8.ffn_up.weight\n",
      "create_tensor: loading tensor blk.9.attn_norm.weight\n",
      "create_tensor: loading tensor blk.9.attn_q.weight\n",
      "create_tensor: loading tensor blk.9.attn_k.weight\n",
      "create_tensor: loading tensor blk.9.attn_v.weight\n",
      "create_tensor: loading tensor blk.9.attn_output.weight\n",
      "create_tensor: loading tensor blk.9.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.9.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.9.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.9.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.9.ffn_down.weight\n",
      "create_tensor: loading tensor blk.9.ffn_up.weight\n",
      "create_tensor: loading tensor blk.10.attn_norm.weight\n",
      "create_tensor: loading tensor blk.10.attn_q.weight\n",
      "create_tensor: loading tensor blk.10.attn_k.weight\n",
      "create_tensor: loading tensor blk.10.attn_v.weight\n",
      "create_tensor: loading tensor blk.10.attn_output.weight\n",
      "create_tensor: loading tensor blk.10.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.10.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.10.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.10.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.10.ffn_down.weight\n",
      "create_tensor: loading tensor blk.10.ffn_up.weight\n",
      "create_tensor: loading tensor blk.11.attn_norm.weight\n",
      "create_tensor: loading tensor blk.11.attn_q.weight\n",
      "create_tensor: loading tensor blk.11.attn_k.weight\n",
      "create_tensor: loading tensor blk.11.attn_v.weight\n",
      "create_tensor: loading tensor blk.11.attn_output.weight\n",
      "create_tensor: loading tensor blk.11.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.11.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.11.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.11.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.11.ffn_down.weight\n",
      "create_tensor: loading tensor blk.11.ffn_up.weight\n",
      "create_tensor: loading tensor blk.12.attn_norm.weight\n",
      "create_tensor: loading tensor blk.12.attn_q.weight\n",
      "create_tensor: loading tensor blk.12.attn_k.weight\n",
      "create_tensor: loading tensor blk.12.attn_v.weight\n",
      "create_tensor: loading tensor blk.12.attn_output.weight\n",
      "create_tensor: loading tensor blk.12.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.12.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.12.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.12.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.12.ffn_down.weight\n",
      "create_tensor: loading tensor blk.12.ffn_up.weight\n",
      "create_tensor: loading tensor blk.13.attn_norm.weight\n",
      "create_tensor: loading tensor blk.13.attn_q.weight\n",
      "create_tensor: loading tensor blk.13.attn_k.weight\n",
      "create_tensor: loading tensor blk.13.attn_v.weight\n",
      "create_tensor: loading tensor blk.13.attn_output.weight\n",
      "create_tensor: loading tensor blk.13.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.13.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.13.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.13.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.13.ffn_down.weight\n",
      "create_tensor: loading tensor blk.13.ffn_up.weight\n",
      "create_tensor: loading tensor blk.14.attn_norm.weight\n",
      "create_tensor: loading tensor blk.14.attn_q.weight\n",
      "create_tensor: loading tensor blk.14.attn_k.weight\n",
      "create_tensor: loading tensor blk.14.attn_v.weight\n",
      "create_tensor: loading tensor blk.14.attn_output.weight\n",
      "create_tensor: loading tensor blk.14.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.14.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.14.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.14.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.14.ffn_down.weight\n",
      "create_tensor: loading tensor blk.14.ffn_up.weight\n",
      "create_tensor: loading tensor blk.15.attn_norm.weight\n",
      "create_tensor: loading tensor blk.15.attn_q.weight\n",
      "create_tensor: loading tensor blk.15.attn_k.weight\n",
      "create_tensor: loading tensor blk.15.attn_v.weight\n",
      "create_tensor: loading tensor blk.15.attn_output.weight\n",
      "create_tensor: loading tensor blk.15.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.15.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.15.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.15.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.15.ffn_down.weight\n",
      "create_tensor: loading tensor blk.15.ffn_up.weight\n",
      "create_tensor: loading tensor blk.16.attn_norm.weight\n",
      "create_tensor: loading tensor blk.16.attn_q.weight\n",
      "create_tensor: loading tensor blk.16.attn_k.weight\n",
      "create_tensor: loading tensor blk.16.attn_v.weight\n",
      "create_tensor: loading tensor blk.16.attn_output.weight\n",
      "create_tensor: loading tensor blk.16.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.16.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.16.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.16.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.16.ffn_down.weight\n",
      "create_tensor: loading tensor blk.16.ffn_up.weight\n",
      "create_tensor: loading tensor blk.17.attn_norm.weight\n",
      "create_tensor: loading tensor blk.17.attn_q.weight\n",
      "create_tensor: loading tensor blk.17.attn_k.weight\n",
      "create_tensor: loading tensor blk.17.attn_v.weight\n",
      "create_tensor: loading tensor blk.17.attn_output.weight\n",
      "create_tensor: loading tensor blk.17.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.17.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.17.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.17.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.17.ffn_down.weight\n",
      "create_tensor: loading tensor blk.17.ffn_up.weight\n",
      "create_tensor: loading tensor blk.18.attn_norm.weight\n",
      "create_tensor: loading tensor blk.18.attn_q.weight\n",
      "create_tensor: loading tensor blk.18.attn_k.weight\n",
      "create_tensor: loading tensor blk.18.attn_v.weight\n",
      "create_tensor: loading tensor blk.18.attn_output.weight\n",
      "create_tensor: loading tensor blk.18.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.18.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.18.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.18.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.18.ffn_down.weight\n",
      "create_tensor: loading tensor blk.18.ffn_up.weight\n",
      "create_tensor: loading tensor blk.19.attn_norm.weight\n",
      "create_tensor: loading tensor blk.19.attn_q.weight\n",
      "create_tensor: loading tensor blk.19.attn_k.weight\n",
      "create_tensor: loading tensor blk.19.attn_v.weight\n",
      "create_tensor: loading tensor blk.19.attn_output.weight\n",
      "create_tensor: loading tensor blk.19.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.19.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.19.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.19.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.19.ffn_down.weight\n",
      "create_tensor: loading tensor blk.19.ffn_up.weight\n",
      "create_tensor: loading tensor blk.20.attn_norm.weight\n",
      "create_tensor: loading tensor blk.20.attn_q.weight\n",
      "create_tensor: loading tensor blk.20.attn_k.weight\n",
      "create_tensor: loading tensor blk.20.attn_v.weight\n",
      "create_tensor: loading tensor blk.20.attn_output.weight\n",
      "create_tensor: loading tensor blk.20.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.20.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.20.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.20.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.20.ffn_down.weight\n",
      "create_tensor: loading tensor blk.20.ffn_up.weight\n",
      "create_tensor: loading tensor blk.21.attn_norm.weight\n",
      "create_tensor: loading tensor blk.21.attn_q.weight\n",
      "create_tensor: loading tensor blk.21.attn_k.weight\n",
      "create_tensor: loading tensor blk.21.attn_v.weight\n",
      "create_tensor: loading tensor blk.21.attn_output.weight\n",
      "create_tensor: loading tensor blk.21.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.21.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.21.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.21.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.21.ffn_down.weight\n",
      "create_tensor: loading tensor blk.21.ffn_up.weight\n",
      "create_tensor: loading tensor blk.22.attn_norm.weight\n",
      "create_tensor: loading tensor blk.22.attn_q.weight\n",
      "create_tensor: loading tensor blk.22.attn_k.weight\n",
      "create_tensor: loading tensor blk.22.attn_v.weight\n",
      "create_tensor: loading tensor blk.22.attn_output.weight\n",
      "create_tensor: loading tensor blk.22.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.22.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.22.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.22.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.22.ffn_down.weight\n",
      "create_tensor: loading tensor blk.22.ffn_up.weight\n",
      "create_tensor: loading tensor blk.23.attn_norm.weight\n",
      "create_tensor: loading tensor blk.23.attn_q.weight\n",
      "create_tensor: loading tensor blk.23.attn_k.weight\n",
      "create_tensor: loading tensor blk.23.attn_v.weight\n",
      "create_tensor: loading tensor blk.23.attn_output.weight\n",
      "create_tensor: loading tensor blk.23.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.23.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.23.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.23.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.23.ffn_down.weight\n",
      "create_tensor: loading tensor blk.23.ffn_up.weight\n",
      "create_tensor: loading tensor blk.24.attn_norm.weight\n",
      "create_tensor: loading tensor blk.24.attn_q.weight\n",
      "create_tensor: loading tensor blk.24.attn_k.weight\n",
      "create_tensor: loading tensor blk.24.attn_v.weight\n",
      "create_tensor: loading tensor blk.24.attn_output.weight\n",
      "create_tensor: loading tensor blk.24.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.24.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.24.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.24.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.24.ffn_down.weight\n",
      "create_tensor: loading tensor blk.24.ffn_up.weight\n",
      "create_tensor: loading tensor blk.25.attn_norm.weight\n",
      "create_tensor: loading tensor blk.25.attn_q.weight\n",
      "create_tensor: loading tensor blk.25.attn_k.weight\n",
      "create_tensor: loading tensor blk.25.attn_v.weight\n",
      "create_tensor: loading tensor blk.25.attn_output.weight\n",
      "create_tensor: loading tensor blk.25.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.25.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.25.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.25.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.25.ffn_down.weight\n",
      "create_tensor: loading tensor blk.25.ffn_up.weight\n",
      "create_tensor: loading tensor blk.26.attn_norm.weight\n",
      "create_tensor: loading tensor blk.26.attn_q.weight\n",
      "create_tensor: loading tensor blk.26.attn_k.weight\n",
      "create_tensor: loading tensor blk.26.attn_v.weight\n",
      "create_tensor: loading tensor blk.26.attn_output.weight\n",
      "create_tensor: loading tensor blk.26.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.26.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.26.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.26.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.26.ffn_down.weight\n",
      "create_tensor: loading tensor blk.26.ffn_up.weight\n",
      "create_tensor: loading tensor blk.27.attn_norm.weight\n",
      "create_tensor: loading tensor blk.27.attn_q.weight\n",
      "create_tensor: loading tensor blk.27.attn_k.weight\n",
      "create_tensor: loading tensor blk.27.attn_v.weight\n",
      "create_tensor: loading tensor blk.27.attn_output.weight\n",
      "create_tensor: loading tensor blk.27.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.27.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.27.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.27.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.27.ffn_down.weight\n",
      "create_tensor: loading tensor blk.27.ffn_up.weight\n",
      "create_tensor: loading tensor blk.28.attn_norm.weight\n",
      "create_tensor: loading tensor blk.28.attn_q.weight\n",
      "create_tensor: loading tensor blk.28.attn_k.weight\n",
      "create_tensor: loading tensor blk.28.attn_v.weight\n",
      "create_tensor: loading tensor blk.28.attn_output.weight\n",
      "create_tensor: loading tensor blk.28.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.28.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.28.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.28.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.28.ffn_down.weight\n",
      "create_tensor: loading tensor blk.28.ffn_up.weight\n",
      "create_tensor: loading tensor blk.29.attn_norm.weight\n",
      "create_tensor: loading tensor blk.29.attn_q.weight\n",
      "create_tensor: loading tensor blk.29.attn_k.weight\n",
      "create_tensor: loading tensor blk.29.attn_v.weight\n",
      "create_tensor: loading tensor blk.29.attn_output.weight\n",
      "create_tensor: loading tensor blk.29.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.29.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.29.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.29.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.29.ffn_down.weight\n",
      "create_tensor: loading tensor blk.29.ffn_up.weight\n",
      "create_tensor: loading tensor blk.30.attn_norm.weight\n",
      "create_tensor: loading tensor blk.30.attn_q.weight\n",
      "create_tensor: loading tensor blk.30.attn_k.weight\n",
      "create_tensor: loading tensor blk.30.attn_v.weight\n",
      "create_tensor: loading tensor blk.30.attn_output.weight\n",
      "create_tensor: loading tensor blk.30.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.30.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.30.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.30.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.30.ffn_down.weight\n",
      "create_tensor: loading tensor blk.30.ffn_up.weight\n",
      "create_tensor: loading tensor blk.31.attn_norm.weight\n",
      "create_tensor: loading tensor blk.31.attn_q.weight\n",
      "create_tensor: loading tensor blk.31.attn_k.weight\n",
      "create_tensor: loading tensor blk.31.attn_v.weight\n",
      "create_tensor: loading tensor blk.31.attn_output.weight\n",
      "create_tensor: loading tensor blk.31.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.31.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.31.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.31.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.31.ffn_down.weight\n",
      "create_tensor: loading tensor blk.31.ffn_up.weight\n",
      "create_tensor: loading tensor blk.32.attn_norm.weight\n",
      "create_tensor: loading tensor blk.32.attn_q.weight\n",
      "create_tensor: loading tensor blk.32.attn_k.weight\n",
      "create_tensor: loading tensor blk.32.attn_v.weight\n",
      "create_tensor: loading tensor blk.32.attn_output.weight\n",
      "create_tensor: loading tensor blk.32.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.32.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.32.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.32.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.32.ffn_down.weight\n",
      "create_tensor: loading tensor blk.32.ffn_up.weight\n",
      "create_tensor: loading tensor blk.33.attn_norm.weight\n",
      "create_tensor: loading tensor blk.33.attn_q.weight\n",
      "create_tensor: loading tensor blk.33.attn_k.weight\n",
      "create_tensor: loading tensor blk.33.attn_v.weight\n",
      "create_tensor: loading tensor blk.33.attn_output.weight\n",
      "create_tensor: loading tensor blk.33.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.33.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.33.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.33.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.33.ffn_down.weight\n",
      "create_tensor: loading tensor blk.33.ffn_up.weight\n",
      "create_tensor: loading tensor blk.34.attn_norm.weight\n",
      "create_tensor: loading tensor blk.34.attn_q.weight\n",
      "create_tensor: loading tensor blk.34.attn_k.weight\n",
      "create_tensor: loading tensor blk.34.attn_v.weight\n",
      "create_tensor: loading tensor blk.34.attn_output.weight\n",
      "create_tensor: loading tensor blk.34.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.34.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.34.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.34.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.34.ffn_down.weight\n",
      "create_tensor: loading tensor blk.34.ffn_up.weight\n",
      "create_tensor: loading tensor blk.35.attn_norm.weight\n",
      "create_tensor: loading tensor blk.35.attn_q.weight\n",
      "create_tensor: loading tensor blk.35.attn_k.weight\n",
      "create_tensor: loading tensor blk.35.attn_v.weight\n",
      "create_tensor: loading tensor blk.35.attn_output.weight\n",
      "create_tensor: loading tensor blk.35.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.35.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.35.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.35.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.35.ffn_down.weight\n",
      "create_tensor: loading tensor blk.35.ffn_up.weight\n",
      "create_tensor: loading tensor blk.36.attn_norm.weight\n",
      "create_tensor: loading tensor blk.36.attn_q.weight\n",
      "create_tensor: loading tensor blk.36.attn_k.weight\n",
      "create_tensor: loading tensor blk.36.attn_v.weight\n",
      "create_tensor: loading tensor blk.36.attn_output.weight\n",
      "create_tensor: loading tensor blk.36.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.36.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.36.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.36.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.36.ffn_down.weight\n",
      "create_tensor: loading tensor blk.36.ffn_up.weight\n",
      "create_tensor: loading tensor blk.37.attn_norm.weight\n",
      "create_tensor: loading tensor blk.37.attn_q.weight\n",
      "create_tensor: loading tensor blk.37.attn_k.weight\n",
      "create_tensor: loading tensor blk.37.attn_v.weight\n",
      "create_tensor: loading tensor blk.37.attn_output.weight\n",
      "create_tensor: loading tensor blk.37.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.37.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.37.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.37.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.37.ffn_down.weight\n",
      "create_tensor: loading tensor blk.37.ffn_up.weight\n",
      "create_tensor: loading tensor blk.38.attn_norm.weight\n",
      "create_tensor: loading tensor blk.38.attn_q.weight\n",
      "create_tensor: loading tensor blk.38.attn_k.weight\n",
      "create_tensor: loading tensor blk.38.attn_v.weight\n",
      "create_tensor: loading tensor blk.38.attn_output.weight\n",
      "create_tensor: loading tensor blk.38.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.38.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.38.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.38.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.38.ffn_down.weight\n",
      "create_tensor: loading tensor blk.38.ffn_up.weight\n",
      "create_tensor: loading tensor blk.39.attn_norm.weight\n",
      "create_tensor: loading tensor blk.39.attn_q.weight\n",
      "create_tensor: loading tensor blk.39.attn_k.weight\n",
      "create_tensor: loading tensor blk.39.attn_v.weight\n",
      "create_tensor: loading tensor blk.39.attn_output.weight\n",
      "create_tensor: loading tensor blk.39.attn_k_norm.weight\n",
      "create_tensor: loading tensor blk.39.attn_q_norm.weight\n",
      "create_tensor: loading tensor blk.39.ffn_norm.weight\n",
      "create_tensor: loading tensor blk.39.ffn_gate.weight\n",
      "create_tensor: loading tensor blk.39.ffn_down.weight\n",
      "create_tensor: loading tensor blk.39.ffn_up.weight\n",
      "load_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type Vulkan_Host, using CPU instead\n",
      "load_tensors: offloading 40 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 41/41 layers to GPU\n",
      "load_tensors:      Vulkan0 model buffer size =  9511.75 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   510.04 MiB\n",
      "..........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = auto\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context: Vulkan_Host  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache: layer   0: dev = Vulkan0\n",
      "llama_kv_cache: layer   1: dev = Vulkan0\n",
      "llama_kv_cache: layer   2: dev = Vulkan0\n",
      "llama_kv_cache: layer   3: dev = Vulkan0\n",
      "llama_kv_cache: layer   4: dev = Vulkan0\n",
      "llama_kv_cache: layer   5: dev = Vulkan0\n",
      "llama_kv_cache: layer   6: dev = Vulkan0\n",
      "llama_kv_cache: layer   7: dev = Vulkan0\n",
      "llama_kv_cache: layer   8: dev = Vulkan0\n",
      "llama_kv_cache: layer   9: dev = Vulkan0\n",
      "llama_kv_cache: layer  10: dev = Vulkan0\n",
      "llama_kv_cache: layer  11: dev = Vulkan0\n",
      "llama_kv_cache: layer  12: dev = Vulkan0\n",
      "llama_kv_cache: layer  13: dev = Vulkan0\n",
      "llama_kv_cache: layer  14: dev = Vulkan0\n",
      "llama_kv_cache: layer  15: dev = Vulkan0\n",
      "llama_kv_cache: layer  16: dev = Vulkan0\n",
      "llama_kv_cache: layer  17: dev = Vulkan0\n",
      "llama_kv_cache: layer  18: dev = Vulkan0\n",
      "llama_kv_cache: layer  19: dev = Vulkan0\n",
      "llama_kv_cache: layer  20: dev = Vulkan0\n",
      "llama_kv_cache: layer  21: dev = Vulkan0\n",
      "llama_kv_cache: layer  22: dev = Vulkan0\n",
      "llama_kv_cache: layer  23: dev = Vulkan0\n",
      "llama_kv_cache: layer  24: dev = Vulkan0\n",
      "llama_kv_cache: layer  25: dev = Vulkan0\n",
      "llama_kv_cache: layer  26: dev = Vulkan0\n",
      "llama_kv_cache: layer  27: dev = Vulkan0\n",
      "llama_kv_cache: layer  28: dev = Vulkan0\n",
      "llama_kv_cache: layer  29: dev = Vulkan0\n",
      "llama_kv_cache: layer  30: dev = Vulkan0\n",
      "llama_kv_cache: layer  31: dev = Vulkan0\n",
      "llama_kv_cache: layer  32: dev = Vulkan0\n",
      "llama_kv_cache: layer  33: dev = Vulkan0\n",
      "llama_kv_cache: layer  34: dev = Vulkan0\n",
      "llama_kv_cache: layer  35: dev = Vulkan0\n",
      "llama_kv_cache: layer  36: dev = Vulkan0\n",
      "llama_kv_cache: layer  37: dev = Vulkan0\n",
      "llama_kv_cache: layer  38: dev = Vulkan0\n",
      "llama_kv_cache: layer  39: dev = Vulkan0\n",
      "llama_kv_cache:    Vulkan0 KV buffer size =   640.00 MiB\n",
      "llama_kv_cache: size =  640.00 MiB (  4096 cells,  40 layers,  1/1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 3544\n",
      "llama_context: reserving full memory module\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "llama_context: Flash Attention was auto, set to enabled\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:    Vulkan0 compute buffer size =   306.75 MiB\n",
      "llama_context: Vulkan_Host compute buffer size =    18.01 MiB\n",
      "llama_context: graph nodes  = 1407\n",
      "llama_context: graph splits = 2\n",
      "clear_adapter_lora: call\n",
      "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
      "common_init_from_params: added <|im_end|> logit bias = -inf\n",
      "common_init_from_params: added <|fim_pad|> logit bias = -inf\n",
      "common_init_from_params: added <|repo_name|> logit bias = -inf\n",
      "common_init_from_params: added <|file_sep|> logit bias = -inf\n",
      "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "set_warmup: value = 1\n",
      "set_warmup: value = 0\n",
      "srv          init: initializing slots, n_slots = 1\n",
      "slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\n",
      "slot        reset: id  0 | task -1 | \n",
      "srv          init: Enable thinking? 1\n",
      "main: model loaded\n",
      "main: chat template, chat_template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for index in range(ns.last_query_index, -1, -1) %}\n",
      "    {%- set message = messages[index] %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not('<tool_response>' in message.content and '</tool_response>' in message.content) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}, example_format: '<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://127.0.0.1:5175 - starting the main loop\n",
      "que    start_loop: processing new tasks\n",
      "que    start_loop: update slots\n",
      "srv  update_slots: all slots are idle\n",
      "srv  kv_cache_cle: clearing KV cache\n",
      "que    start_loop: waiting for new tasks\n"
     ]
    }
   ],
   "source": [
    "def log_output(pipe):\n",
    "    for line in pipe:\n",
    "        print(line, end=\"\", flush=True)\n",
    "\n",
    "thread = threading.Thread(target=log_output, args=(process.stdout,), daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7a6754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.stdin.write(\"I'm glad you're working properly'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62799e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "def send_prompt(prompt):\n",
    "    history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    request = {\"messages\": history, \"max_tokens\": -1, \"stream\": True}\n",
    "    response = requests.post(url, json=request, stream=True)\n",
    "    client = sseclient.SSEClient(response)\n",
    "    reply = \"\"\n",
    "    final_stats = 0\n",
    "    for event in client.events():\n",
    "        if event.data.strip() == \"[DONE]\":\n",
    "            break\n",
    "        try:\n",
    "            chunk = json.loads(event.data)\n",
    "            choices = chunk.get('choices', [])\n",
    "            if not choices:\n",
    "                if \"usage\" in chunk or \"timings\" in chunk:\n",
    "                    final_stats = chunk\n",
    "                    yield chunk\n",
    "                continue\n",
    "            delta = choices[0].get('delta', {})\n",
    "            token = delta.get('content')\n",
    "            if token:\n",
    "                reply += token\n",
    "                yield token\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            continue\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply.split(\"</think>\")[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345d34d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object send_prompt at 0x7f058e5b8170>\n"
     ]
    }
   ],
   "source": [
    "reply_tokens = []\n",
    "responsese = send_prompt(\"What are the best 5 frameworks for language management in JS?\")\n",
    "print(responsese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78678374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for the best 5 frameworks for language management in JavaScript. Let me think about this.\n",
      "\n",
      "First, I need to clarify what they mean by \"language management.\" That could be internationalization (i18n) and localization (l10n), right? So frameworks that help with translating text, formatting dates, numbers, etc., for different languages.\n",
      "\n",
      "Now, I should recall the popular JS libraries for i18n. The first one that comes to mind is i18next. It's widely used and has good support for React and other frameworks. Then there's FormatJS, which is part of the React ecosystem and includes tools like react-intl. \n",
      "\n",
      "Another one is Transifex, but wait, Transifex is more of a translation management system rather than a framework. Maybe the user is looking for actual JS libraries, not CMS or platforms. So I should focus on libraries.\n",
      "\n",
      "Then there's Preact's use of internationalization, but maybe that's too specific. Oh, and there's also Locale, but I think that's more of a utility library. Maybe I should check if there are others like Lodash's i18n or something else.\n",
      "\n",
      "Wait, let me list out the top ones. i18next, FormatJS (including react-intl), and then maybe something like Lingui or React-i18next. Oh, React-i18next is a wrapper around i18next for React. So maybe that's a separate entry. Also, there's the internationalization library from the React team, which is part of FormatJS.\n",
      "\n",
      "Let me make sure I'm not missing any. There's also the Globalize library, but I think it's deprecated now. So maybe the top five are:\n",
      "\n",
      "1. i18next\n",
      "2. FormatJS (including react-intl)\n",
      "3. React-i18next\n",
      "4. Lingui\n",
      "5. Transifex (if they count as a framework) or maybe something else like SprintfJS or date-fns for formatting.\n",
      "\n",
      "Wait, the user said frameworks. Transifex is more of a service. Maybe the top five are i18next, FormatJS, React-i18next, Lingui, and something else. Oh, maybe the Internationalization API in JavaScript, which is part of the browser, but that's more of a built-in feature. Or maybe the ICU library, but that's more for Java.\n",
      "\n",
      "Alternatively, I could consider libraries that are framework-agnostic. So i18next, FormatJS (which includes tools for React and others), Lingui, and maybe something like Locale. Wait, I think the top ones are:\n",
      "\n",
      "1. i18next\n",
      "2. FormatJS (with react-intl)\n",
      "3. React-i18next (as a separate entry)\n",
      "4. Lingui\n",
      "5. SprintfJS or another library for string formatting.\n",
      "\n",
      "Alternatively, maybe the answer includes the main ones and explains each. Also, I should check if there are any newer libraries that have gained popularity. Let me confirm the current state as of 2023.\n",
      "\n",
      "Yes, i18next is definitely a top choice. FormatJS with react-intl is popular in React apps. Lingui is another modern library. Then there's the internationalization plugin for Vue, but the user didn't specify a framework. So maybe the answer should be general JS, not specific to React or Vue. So React-i18next is a React-specific wrapper, but maybe the core i18next is more general.\n",
      "\n",
      "Also, there's the internationalization API in JavaScript, but that's part of the browser and not a framework. So the best five would be:\n",
      "\n",
      "1. i18next\n",
      "2. FormatJS (including react-intl)\n",
      "3. Lingui\n",
      "4. SprintfJS\n",
      "5. Globalize (though it's deprecated, maybe not include that)\n",
      "\n",
      "Wait, SprintfJS might be too niche. Maybe include something like date-fns for date formatting as part of i18n. But the user asked for frameworks for language management, which includes text translation, date, number formatting, etc.\n",
      "\n",
      "Alternatively, maybe the answer should include:\n",
      "\n",
      "1. i18next\n",
      "2. react-intl (part of FormatJS)\n",
      "3. Lingui\n",
      "4. Transifex (if they consider it a framework)\n",
      "5. FormatJS (as a broader ecosystem)\n",
      "\n",
      "Hmm, maybe the top five are:\n",
      "\n",
      "1. i18next\n",
      "2. react-intl (FormatJS)\n",
      "3. Lingui\n",
      "4. i18n.js (a lightweight library)\n",
      "5. Transifex (as a platform, but maybe not a framework)\n",
      "\n",
      "Alternatively, check if there are other libraries. Maybe the answer should mention the main ones with a bit of explanation for each. I think that's the way to go.\n",
      "</think>\n",
      "\n",
      "Here are the **top 5 frameworks/libraries for language management (internationalization/localization, i18n/l10n)** in JavaScript, tailored for flexibility and popularity in the JS ecosystem as of 2023:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. [i18next](https://www.i18next.com/)**\n",
      "- **Overview**: A powerful, modular, and widely-used i18n framework for JavaScript applications. It supports multiple frameworks (React, Vue, Angular, etc.) and provides tools for translation, pluralization, and date/number formatting.\n",
      "- **Key Features**:\n",
      "  - Supports nested translation keys.\n",
      "  - Works with React via `react-i18next`.\n",
      "  - Uses JSON-based translation files.\n",
      "  - Backend integration for dynamic loading.\n",
      "- **Use Case**: Ideal for large-scale applications requiring robust i18n support.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. [FormatJS (react-intl)](https://formatjs.io/docs/react-intl/)**\n",
      "- **Overview**: A suite of tools by the React team for i18n in React apps. The core is `react-intl`, which integrates with React components.\n",
      "- **Key Features**:\n",
      "  - Component-based translation (e.g., `<FormattedMessage>`).\n",
      "  - Supports pluralization, date/time, and number formatting.\n",
      "  - Works with `create-react-app` and other build tools.\n",
      "- **Use Case**: Perfect for React applications needing localized UI and data formatting.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. [Lingui](https://lingui.js.org/)**\n",
      "- **Overview**: A modern i18n framework with a focus on developer experience. It uses a declarative API and integrates well with React and Vue.\n",
      "- **Key Features**:\n",
      "  - Uses `@lingui/core` and `@lingui/react` for React apps.\n",
      "  - Automatic message extraction for translation.\n",
      "  - Supports pluralization, interpolation, and formatting.\n",
      "- **Use Case**: Great for projects using modern JS tooling (Babel, Webpack) and React/Vue.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. [Transifex](https://www.transifex.com/)**\n",
      "- **Overview**: A **translation management platform** that integrates with code repositories. While not a framework itself, it works with i18n tools like `i18next` or `react-intl` to automate translation workflows.\n",
      "- **Key Features**:\n",
      "  - Collaborative translation with teams.\n",
      "  - Syncs translation files with codebases.\n",
      "  - Supports real-time updates and version control.\n",
      "- **Use Case**: Ideal for teams needing collaboration tools for managing translations.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. [date-fns-i18n](https://date-fns.org/docs/i18n)** (or [SprintfJS](https://www.i18next.com/))\n",
      "- **Overview**: Libraries for formatting dates, numbers, and strings in a localized way. These are often used alongside i18n frameworks.\n",
      "- **Key Features**:\n",
      "  - `date-fns-i18n`: Integrates with `date-fns` for localized date formatting.\n",
      "  - `SprintfJS`: Implements `printf`-style string formatting with i18n support.\n",
      "- **Use Case**: Complements other i18n libraries for specific formatting needs.\n",
      "\n",
      "---\n",
      "\n",
      "### **Honorable Mentions**\n",
      "- **Vue I18n**: For Vue.js applications, though it’s framework-specific.\n",
      "- **Globalize**: A legacy library (now deprecated) for i18n in JS.\n",
      "- **ICU (International Components for Unicode)**: A robust but low-level library for global formatting.\n",
      "\n",
      "---\n",
      "\n",
      "### **Choosing the Right Tool**\n",
      "- **General-purpose apps**: `i18next` or `Lingui`.\n",
      "- **React apps**: `react-intl` (FormatJS) or `react-i18next`.\n",
      "- **Translation workflows**: Combine `i18next` with **Transifex** for team collaboration.\n",
      "- **Date/number formatting**: Use `date-fns-i18n` or `react-intl`'s built-in formatters.\n",
      "\n",
      "Let me know if you need help with a specific framework! 🌍\n",
      "Speed:  48.93 t/s\n"
     ]
    }
   ],
   "source": [
    "reply_tokens = []\n",
    "responsese = send_prompt(\"What are the best 5 frameworks for language management in JS?\")\n",
    "stats = 0\n",
    "for token in responsese:\n",
    "    if isinstance(token, dict):\n",
    "        stats = token\n",
    "        continue\n",
    "    print(token, end='', flush=True)\n",
    "    reply_tokens.append(token)\n",
    "\n",
    "print(\"\\nSpeed: \", round(stats['timings']['predicted_per_second'], 2), \"t/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_LMnGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
