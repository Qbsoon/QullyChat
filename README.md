# QullyChat

QullyChat app strives to offer best AI chat experience as possible, offering as many options to the user as possible and both LLM and Diffusion responses in the same chat.
The idea was born out of my frustration with some things not available in LMStudio and with some ways LMStudio does things.

## Road-to-v1 TO-DO

- [x] Implement core chat functionality
- [x] Implement LLM model list
- [ ] Generate requirements.txt
- [ ] Add settings tab
    - [ ] Settings profiles
    - [ ] Default settings
    - [ ] Model-bound settings
    - [ ] System prompt profiles
    - [ ] System prompt changing in chat
- [ ] LLM changing in chat
- [ ] Starting llama.cpp server in app
- [ ] Implement Image Generation in chat
- [ ] Implement GenAI model list
- [ ] Adjust settings tab for GenAI models
- [ ] Package first release for github
- [ ] Make looks adjustments
    - [ ] ???

## v1-ready TO-DO

- [ ] Add installation instructions
- [ ] Document usage examples
- [ ] Implement model evaluation module
- [ ] Add contributing guidelines
- [ ] Feature overview

## After-v1 TO-DO

- [ ] Add Huggingface support